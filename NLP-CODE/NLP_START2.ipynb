{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5476420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\prish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\prish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\prish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\prish\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44fb76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello World , this is a very interesting application and implementation of machine learning . This is okay.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be96970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting sentence to paragraphs\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23cfc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents =sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e26bc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a068d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World , this is a very interesting application and implementation of machine learning .',\n",
       " 'This is okay.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f5943b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sentence is: Hello World , this is a very interesting application and implementation of machine learning .\n",
      "the sentence is: This is okay.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(\"the sentence is:\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a93b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting paragraph to words\n",
    "## converting sentences to words\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad3fbfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'World',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'interesting',\n",
       " 'application',\n",
       " 'and',\n",
       " 'implementation',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'okay',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70da454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a94dcb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'World',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'interesting',\n",
       " 'application',\n",
       " 'and',\n",
       " 'implementation',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'okay',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)  #punctuation also treated as seperate word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43860212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5264051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b86e629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'World',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'interesting',\n",
       " 'application',\n",
       " 'and',\n",
       " 'implementation',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'okay',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)  #fullstop not treated as a seperate word, it is included in the previous word, but in the last word , it is considered as a seperate word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a179cf1",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2092a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to analyse the word stem ,have one word instead ofhaving similar kind of words\n",
    "# used for classification problems , like to analyse reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ef063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\",\"eaten\",\"will eat\",\"eat\",\"sing\",\"singing\",\"dance\",\"write\",\"writing\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420b299f",
   "metadata": {},
   "source": [
    "PORTERSTEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59441157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c977659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93826478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating is interpreted as ------> eat\n",
      "eaten is interpreted as ------> eaten\n",
      "will eat is interpreted as ------> will eat\n",
      "eat is interpreted as ------> eat\n",
      "sing is interpreted as ------> sing\n",
      "singing is interpreted as ------> sing\n",
      "dance is interpreted as ------> danc\n",
      "write is interpreted as ------> write\n",
      "writing is interpreted as ------> write\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \" is interpreted as ------> \"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a582655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congrat'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#disadvantage is\n",
    "stemming.stem(\"congrats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8fb2f",
   "metadata": {},
   "source": [
    "REGEXPSTEMMER CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8617d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes a single expression and removes any prefix or suffix that matches the expression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b12ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "074075b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$',min=4)  #only when ing is in the last , it can be removed to get stem word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f86434df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3e6562",
   "metadata": {},
   "source": [
    "# SNOWBALL STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bbcd212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ffb5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cee854f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eat\n",
      "eaten--->eaten\n",
      "will eat--->will eat\n",
      "eat--->eat\n",
      "sing--->sing\n",
      "singing--->sing\n",
      "dance--->danc\n",
      "write--->write\n",
      "writing--->write\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"--->\" + snowballstemmer.stem(word))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7e2a9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#porterstemmer\n",
    "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ed412f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#snowballstemmer\n",
    "snowballstemmer.stem(\"fairly\"),snowballstemmer.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9e2865",
   "metadata": {},
   "source": [
    "# LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30a5d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization give soutput as lemma which is the root word , not the root stem\n",
    "# we will be getting a valid word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c1448a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88fe247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "444841d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2bb6367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b9bd00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('going','v')  #default is n for noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3a3fe53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goes'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goes\",pos=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "587fb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98aa1be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67421a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')  #words that are commonly used and have little value  when it comes to answer queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13015450",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"In this instance, the predetermined rule is followed by the rule-based POS tagger to label words. “Noun” tags are applied to words like “presentation,” “achievements,” and “development” because of the aforementioned restriction. Despite the simplicity of this example, rule-based taggers may handle a broad variety of linguistic patterns by incorporating different rules, which makes the tagging process transparent and comprehensible.\n",
    "\n",
    "2. Transformation Based tagging\n",
    "Transformation-based tagging (TBT) is a part-of-speech (POS) tagging method that uses a set of rules to change the tags that are applied to words inside a text. In contrast, statistical POS tagging uses trained algorithms to predict tags probabilistically, while rule-based POS tagging assigns tags directly based on predefined rules.\n",
    "\n",
    "To change word tags in TBT, a set of rules is created depending on contextual information. A rule could, for example, change a verb’s tag to a noun if it comes after a determiner like “the.” The text is systematically subjected to these criteria, and after each transformation, the tags are updated.\n",
    "\n",
    "When compared to rule-based tagging, TBT can provide higher accuracy, especially when dealing with complex grammatical structures. To attain ideal performance, nevertheless, it might require a large rule set and additional computer power.\n",
    "\n",
    "Consider the transformation rule: Change the tag of a verb to a noun if it follows a determiner like “the.” \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1c193a59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9758558",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "928cfc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying stopwords filter and then stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = \" \".join(words)  #converts all list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e616390d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['instanc , predetermin rule follow rule-ba po tagger label word .',\n",
       " '“ noun ” tag appli word like “ present , ” “ achiev , ” “ develop ” afor restrict .',\n",
       " 'despit simplic exampl , rule-ba tagger may handl broad varieti linguist pattern incorpor differ rule , make tag process transpar comprehen .',\n",
       " '2 .',\n",
       " 'transform base tag transformation-ba tag ( tbt ) part-of-speech ( po ) tag method use set rule chang tag appli word insid text .',\n",
       " 'contrast , statist po tag use train algorithm predict tag probabilist , rule-ba po tag assign tag directli base predefin rule .',\n",
       " 'chang word tag tbt , set rule creat depend contextu inform .',\n",
       " 'rule could , exampl , chang verb ’ tag noun come determin like “ . ” text systemat subject criteria , transform , tag updat .',\n",
       " 'compar rule-ba tag , tbt provid higher accuraci , especi deal complex grammat structur .',\n",
       " 'attain ideal perform , nevertheless , might requir larg rule set addit comput power .',\n",
       " 'consid transform rule : chang tag verb noun follow determin like “ . ”']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8c3d9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "72b66bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = \" \".join(words)  #converts all list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "64ae5b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['instanc , predetermin rule follow rule-ba po tagger label word .',\n",
       " '“ noun ” tag appli word like “ present , ” “ achiev , ” “ develop ” afor restrict .',\n",
       " 'despit simplic exampl , rule-ba tagger may handl broad varieti linguist pattern incorpor differ rule , make tag process transpar comprehen .',\n",
       " '2 .',\n",
       " 'transform base tag transformation-ba tag ( tbt ) part-of-speech ( po ) tag method use set rule chang tag appli word insid text .',\n",
       " 'contrast , statist po tag use train algorithm predict tag probabilist , rule-ba po tag assign tag direct base predefin rule .',\n",
       " 'chang word tag tbt , set rule creat depend contextu inform .',\n",
       " 'rule could , exampl , chang verb ’ tag noun come determin like “ . ” text systemat subject criteria , transform , tag updat .',\n",
       " 'compar rule-ba tag , tbt provid higher accuraci , especi deal complex grammat structur .',\n",
       " 'attain ideal perform , nevertheless , might requir larg rule set addit comput power .',\n",
       " 'consid transform rule : chang tag verb noun follow determin like “ . ”']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences   # all letters become small so that it doesnt distunguish due to capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ca3ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = \" \".join(words)  #converts all list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2dfe2b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in instance , predetermined rule followed rule-based po tagger label word .',\n",
       " '“ noun ” tag applied word like “ presentation , ” “ achievement , ” “ development ” aforementioned restriction .',\n",
       " 'despite simplicity example , rule-based tagger may handle broad variety linguistic pattern incorporating different rule , make tagging process transparent comprehensible .',\n",
       " '2 .',\n",
       " 'transformation based tagging transformation-based tagging ( tbt ) part-of-speech ( po ) tagging method u set rule change tag applied word inside text .',\n",
       " 'in contrast , statistical po tagging u trained algorithm predict tag probabilistically , rule-based po tagging assigns tag directly based predefined rule .',\n",
       " 'to change word tag tbt , set rule created depending contextual information .',\n",
       " 'a rule could , example , change verb ’ tag noun come determiner like “ . ” the text systematically subjected criterion , transformation , tag updated .',\n",
       " 'when compared rule-based tagging , tbt provide higher accuracy , especially dealing complex grammatical structure .',\n",
       " 'to attain ideal performance , nevertheless , might require large rule set additional computer power .',\n",
       " 'consider transformation rule : change tag verb noun follows determiner like “ . ”']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943e37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
